
# ğŸ§  Changi Airport AI Chatbot (Ollama + FastAPI)

This project is a smart chatbot that answers queries about **Changi Airport** and **Jewel Changi** using an open-source LLM (e.g., `llama3`) powered by **Ollama**, deployed on **RunPod**, and served via **FastAPI** on **Render**. It uses RAG (Retrieval-Augmented Generation) with local PDF data.

---

## ğŸ“¦ Features

- ğŸ’¬ Conversational chatbot interface (FastAPI/HTML UI)
- ğŸ§  RAG-enabled using FAISS vector store
- ğŸ“š Custom-trained on Changi + Jewel Changi Airport PDFs
- âš¡ Ollama-powered LLM (e.g., llama3.2:1b)
- â˜ï¸ Ollama hosted on **RunPod GPU**
- ğŸŒ FastAPI backend hosted on **Render**
- ğŸ” Semantic search with `all-MiniLM-L6-v2` embeddings

---

## ğŸ› ï¸ Project Structure

```
chatbot/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ api_server.py         # FastAPI backend entrypoint
â”‚   â”‚   â”œâ”€â”€ chat_ui.py            # HTML + JS-based frontend
â”‚   â”‚   â”œâ”€â”€ final_chatbot1.py     # Core RAG chatbot logic
â”‚   â”‚   â”œâ”€â”€ vector_store.py       # FAISS index creation
â”‚   â”‚   â”œâ”€â”€ scrapers.py           # Website scraper (optional)
â”‚   â”‚   â””â”€â”€ text_processor.py     # PDF parsing & chunking
â”‚   â”œâ”€â”€ vector_store_*.json       # Vector index + stats
â”‚   â””â”€â”€ scraped_data_*.json       # Raw scraped text
```

---

## ğŸš€ Setup Instructions

### Step 1: ğŸ§  Deploy Ollama on RunPod

1. Go to [RunPod.io](https://console.runpod.io/)
2. Click `Deploy a Pod` â†’ Choose **GPU** tab (e.g., NVIDIA T4)
3. Search & select **"Ollama"** community template
4. Expose Port `11434`
5. Deploy & copy your **Public IP**

### Step 2: ğŸ§± Deploy FastAPI on Render

1. Push your code to GitHub
2. Go to [Render](https://render.com)
3. Click **New â†’ Web Service**
4. Connect your repo & set:
   - **Start Command**:  
     ```
     uvicorn data.src.api_server:app --host 0.0.0.0 --port 10000
     ```
   - **Build Command**: `pip install -r requirements.txt`
5. Add ENV var:
   - `OLLAMA_BASE_URL=http://<RunPod-IP>:11434`

---

## ğŸ§ª Local Development

```bash
# Step 1: Install dependencies
pip install -r requirements.txt

# Step 2: Generate vector store
python data/src/vector_store.py

# Step 3: Run chatbot locally (assuming Ollama is running)
python data/src/api_server.py
```

---

## ğŸ”— Usage

Access the chatbot UI at:

```
https://your-render-subdomain.onrender.com
```

Ask questions like:
- â€œWhere is the Rain Vortex?â€
- â€œHow to access Jewel from Terminal 3?â€

---

## ğŸ§  Model & Tech Stack

| Component        | Tech                            |
|------------------|----------------------------------|
| LLM              | Mistral / LLaMA 3 via Ollama     |
| Embedding Model  | `all-MiniLM-L6-v2`               |
| Vector DB        | FAISS                           |
| Backend          | FastAPI                         |
| Frontend         | HTML + JS (Simple UI)           |
| Deployment       | Render + RunPod                 |

---

## ğŸ“„ License

MIT License.  
Data and chatbot logic are tailored for educational and non-commercial use.
